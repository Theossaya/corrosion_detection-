{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14dd67d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "               DAY 1: EDA & DATASET SPLITTING\n",
      "======================================================================\n",
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "Total Images: 561\n",
      "Total Annotations: 3056\n",
      "Categories: 1\n",
      "\n",
      "Category Distribution:\n",
      "  corrosion: 3056 (100.0%)\n",
      "============================================================\n",
      "\n",
      "ANALYZING IMAGE PROPERTIES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 561/561 [00:00<00:00, 2021.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Dimension Stats:\n",
      "  Width: mean=886.96, std=747.15, min=640.00, max=4032.00\n",
      "  Height: mean=969.61, std=993.97, min=640.00, max=4032.00\n",
      "  Aspect Ratio: mean=0.98, std=0.08, min=0.75, max=1.33\n",
      "\n",
      "File Sizes: Mean=205.93 KB, Range=[19.62, 2437.70]\n",
      "\n",
      "ANALYZING ANNOTATIONS...\n",
      "  Mean bbox area: 112063.90\n",
      "  Max per-image annotations: 310\n",
      "\n",
      "GENERATING SAMPLE VISUALIZATIONS...\n",
      "‚úì Saved to: results\\figures\\sample_annotations.png\n",
      "\n",
      "GENERATING DISTRIBUTION PLOTS...\n",
      "‚úì Saved to: results\\figures\\data_distributions.png\n",
      "\n",
      "============================================================\n",
      "CREATING DATA SPLITS\n",
      "============================================================\n",
      "  Train: 392, Val: 84, Test: 85\n",
      "‚úì Saved train annotations to data\\processed\\train_annotations.json\n",
      "‚úì Saved val annotations to data\\processed\\val_annotations.json\n",
      "‚úì Saved test annotations to data\\processed\\test_annotations.json\n",
      "\n",
      "‚úì DAY 1 COMPLETE ‚Äì Visuals in results/figures, splits in data/processed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete EDA and Dataset Splitting Script\n",
    "Analyzes unified dataset and creates stratified (or random) train/val/test splits\n",
    "Handles single-class datasets gracefully (like 'corrosion' only)\n",
    "\n",
    "Run in Jupyter or as standalone script:\n",
    "    python notebooks/01_eda_and_split.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DatasetAnalyzer:\n",
    "    def __init__(self, coco_json_path):\n",
    "        self.coco_path = Path(coco_json_path)\n",
    "        with open(coco_json_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        self.images = self.coco_data.get('images', [])\n",
    "        self.annotations = self.coco_data.get('annotations', [])\n",
    "        self.categories = {c['id']: c['name'] for c in self.coco_data.get('categories', [])}\n",
    "        \n",
    "        # Create lookups\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        for ann in self.annotations:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "    def basic_stats(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total Images: {len(self.images)}\")\n",
    "        print(f\"Total Annotations: {len(self.annotations)}\")\n",
    "        print(f\"Categories: {len(self.categories)}\")\n",
    "        \n",
    "        cat_counts = Counter([ann['category_id'] for ann in self.annotations])\n",
    "        if not cat_counts:\n",
    "            print(\"\\n‚ö† No annotations found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCategory Distribution:\")\n",
    "        for cat_id, count in cat_counts.most_common():\n",
    "            cat_name = self.categories.get(cat_id, 'unknown')\n",
    "            percentage = (count / len(self.annotations)) * 100\n",
    "            print(f\"  {cat_name}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def analyze_images(self):\n",
    "        print(\"\\nANALYZING IMAGE PROPERTIES...\")\n",
    "        widths, heights, aspect_ratios = [], [], []\n",
    "        file_sizes = []\n",
    "        \n",
    "        for img in tqdm(self.images):\n",
    "            widths.append(img.get('width', 0))\n",
    "            heights.append(img.get('height', 0))\n",
    "            if img.get('height', 0) > 0:\n",
    "                aspect_ratios.append(img.get('width', 1) / img.get('height', 1))\n",
    "            \n",
    "            if 'path' in img and Path(img['path']).exists():\n",
    "                file_sizes.append(Path(img['path']).stat().st_size / 1024)  # KB\n",
    "        \n",
    "        stats = {\n",
    "            'Width': {'min': np.min(widths), 'max': np.max(widths), 'mean': np.mean(widths), 'std': np.std(widths)},\n",
    "            'Height': {'min': np.min(heights), 'max': np.max(heights), 'mean': np.mean(heights), 'std': np.std(heights)},\n",
    "            'Aspect Ratio': {'min': np.min(aspect_ratios), 'max': np.max(aspect_ratios), 'mean': np.mean(aspect_ratios), 'std': np.std(aspect_ratios)}\n",
    "        }\n",
    "        \n",
    "        print(\"\\nImage Dimension Stats:\")\n",
    "        for metric, values in stats.items():\n",
    "            print(f\"  {metric}: mean={values['mean']:.2f}, std={values['std']:.2f}, min={values['min']:.2f}, max={values['max']:.2f}\")\n",
    "        \n",
    "        if file_sizes:\n",
    "            print(f\"\\nFile Sizes: Mean={np.mean(file_sizes):.2f} KB, Range=[{np.min(file_sizes):.2f}, {np.max(file_sizes):.2f}]\")\n",
    "        \n",
    "        return stats, widths, heights, aspect_ratios\n",
    "    \n",
    "    def analyze_annotations(self):\n",
    "        print(\"\\nANALYZING ANNOTATIONS...\")\n",
    "        if not self.annotations:\n",
    "            print(\"‚ö† No annotations found ‚Äî skipping.\")\n",
    "            return [], []\n",
    "        \n",
    "        bbox_areas, anns_per_image = [], []\n",
    "        \n",
    "        for img in self.images:\n",
    "            anns = self.img_to_anns[img['id']]\n",
    "            anns_per_image.append(len(anns))\n",
    "            for ann in anns:\n",
    "                w, h = ann['bbox'][2], ann['bbox'][3]\n",
    "                bbox_areas.append(w * h)\n",
    "        \n",
    "        print(f\"  Mean bbox area: {np.mean(bbox_areas):.2f}\")\n",
    "        print(f\"  Max per-image annotations: {max(anns_per_image)}\")\n",
    "        return bbox_areas, anns_per_image\n",
    "    \n",
    "    def visualize_samples(self, num_samples=12, save_dir=\"results/figures\"):\n",
    "        print(\"\\nGENERATING SAMPLE VISUALIZATIONS...\")\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if len(self.images) == 0:\n",
    "            print(\"‚ö† No images to visualize.\")\n",
    "            return\n",
    "        \n",
    "        sample_imgs = np.random.choice(self.images, min(num_samples, len(self.images)), replace=False)\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, img_info in enumerate(sample_imgs):\n",
    "            if 'path' not in img_info or not Path(img_info['path']).exists():\n",
    "                continue\n",
    "            \n",
    "            img = cv2.imread(img_info['path'])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            anns = self.img_to_anns[img_info['id']]\n",
    "            \n",
    "            for ann in anns:\n",
    "                x, y, w, h = map(int, ann['bbox'])\n",
    "                color = (255, 0, 0)  # red box for corrosion\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "                cat_name = self.categories.get(ann['category_id'], 'corrosion')\n",
    "                cv2.putText(img, cat_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(f\"{Path(img_info['file_name']).stem}\\n{len(anns)} annotations\")\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"sample_annotations.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved to: {save_dir / 'sample_annotations.png'}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_distributions(self, widths, heights, bbox_areas, save_dir=\"results/figures\"):\n",
    "        print(\"\\nGENERATING DISTRIBUTION PLOTS...\")\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        axes[0, 0].hist(widths, bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 0].set_title('Image Widths')\n",
    "        \n",
    "        axes[0, 1].hist(heights, bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 1].set_title('Image Heights')\n",
    "        \n",
    "        aspect_ratios = [w / h for w, h in zip(widths, heights) if h > 0]\n",
    "        axes[0, 2].hist(aspect_ratios, bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 2].set_title('Aspect Ratios')\n",
    "        \n",
    "        if bbox_areas:\n",
    "            axes[1, 0].hist(bbox_areas, bins=50, edgecolor='black', alpha=0.7)\n",
    "            axes[1, 0].set_title('Bounding Box Areas')\n",
    "            axes[1, 0].set_yscale('log')\n",
    "        \n",
    "        cat_counts = Counter([ann['category_id'] for ann in self.annotations])\n",
    "        if cat_counts:\n",
    "            axes[1, 1].bar([self.categories[c] for c in cat_counts.keys()], list(cat_counts.values()), edgecolor='black', alpha=0.7)\n",
    "            axes[1, 1].set_title('Category Distribution')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No category data', ha='center', va='center')\n",
    "        \n",
    "        anns_per_img = [len(self.img_to_anns[img['id']]) for img in self.images]\n",
    "        axes[1, 2].hist(anns_per_img, bins=range(max(anns_per_img)+2), edgecolor='black', alpha=0.7)\n",
    "        axes[1, 2].set_title('Annotations per Image')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"data_distributions.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved to: {save_dir / 'data_distributions.png'}\")\n",
    "        plt.close()\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, coco_json_path, output_dir=\"data/processed\"):\n",
    "        self.coco_path = Path(coco_json_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(coco_json_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "    \n",
    "    def stratified_split(self, train_ratio=0.70, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CREATING DATA SPLITS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        images = self.coco_data['images']\n",
    "        annotations = self.coco_data['annotations']\n",
    "        img_to_cats = defaultdict(list)\n",
    "        for ann in annotations:\n",
    "            img_to_cats[ann['image_id']].append(ann['category_id'])\n",
    "        \n",
    "        image_labels = []\n",
    "        for img in images:\n",
    "            cats = img_to_cats[img['id']]\n",
    "            primary_cat = Counter(cats).most_common(1)[0][0] if cats else 0\n",
    "            image_labels.append(primary_cat)\n",
    "        \n",
    "        unique_labels = list(set(image_labels))\n",
    "        if len(unique_labels) < 2:\n",
    "            print(\"‚ö† Only one class detected ‚Äî using random split instead.\")\n",
    "            train_val_imgs, test_imgs = train_test_split(images, test_size=test_ratio, random_state=random_state)\n",
    "            val_ratio_adj = val_ratio / (train_ratio + val_ratio)\n",
    "            train_imgs, val_imgs = train_test_split(train_val_imgs, test_size=val_ratio_adj, random_state=random_state)\n",
    "        else:\n",
    "            train_val_imgs, test_imgs, train_val_labels, test_labels = train_test_split(\n",
    "                images, image_labels, test_size=test_ratio, stratify=image_labels, random_state=random_state)\n",
    "            val_ratio_adj = val_ratio / (train_ratio + val_ratio)\n",
    "            train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "                train_val_imgs, train_val_labels, test_size=val_ratio_adj, stratify=train_val_labels, random_state=random_state)\n",
    "        \n",
    "        splits = {'train': train_imgs, 'val': val_imgs, 'test': test_imgs}\n",
    "        print(f\"  Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n",
    "        return splits\n",
    "    \n",
    "    def save_splits(self, splits):\n",
    "        for split_name, split_imgs in splits.items():\n",
    "            split_img_ids = {img['id'] for img in split_imgs}\n",
    "            split_anns = [ann for ann in self.coco_data['annotations'] if ann['image_id'] in split_img_ids]\n",
    "            split_data = {\n",
    "                'info': self.coco_data['info'],\n",
    "                'images': split_imgs,\n",
    "                'annotations': split_anns,\n",
    "                'categories': self.coco_data['categories']\n",
    "            }\n",
    "            output_path = self.output_dir / f\"{split_name}_annotations.json\"\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(split_data, f, indent=2)\n",
    "            print(f\"‚úì Saved {split_name} annotations to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" \" * 15 + \"DAY 1: EDA & DATASET SPLITTING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    coco_json = \"data/processed/unified/unified_annotations.json\"\n",
    "    if not Path(coco_json).exists():\n",
    "        print(f\"‚úó Missing: {coco_json}\")\n",
    "        return\n",
    "    \n",
    "    analyzer = DatasetAnalyzer(coco_json)\n",
    "    analyzer.basic_stats()\n",
    "    stats, widths, heights, aspect_ratios = analyzer.analyze_images()\n",
    "    bbox_areas, anns_per_img = analyzer.analyze_annotations()\n",
    "    analyzer.visualize_samples(num_samples=12)\n",
    "    analyzer.plot_distributions(widths, heights, bbox_areas)\n",
    "    \n",
    "    splitter = DatasetSplitter(coco_json)\n",
    "    splits = splitter.stratified_split()\n",
    "    splitter.save_splits(splits)\n",
    "    \n",
    "    print(\"\\n‚úì DAY 1 COMPLETE ‚Äì Visuals in results/figures, splits in data/processed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9317f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading unified dataset: C:\\Users\\Blue\\corrosion_detection\\src\\data\\data\\processed\\unified\\unified_annotations.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Blue\\\\corrosion_detection\\\\src\\\\data\\\\data\\\\processed\\\\unified\\\\unified_annotations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# LOAD COCO JSON\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÇ Loading unified dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUNIFIED_JSON\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(UNIFIED_JSON, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     38\u001b[0m     coco_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     40\u001b[0m images \u001b[38;5;241m=\u001b[39m coco_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Blue\\\\corrosion_detection\\\\src\\\\data\\\\data\\\\processed\\\\unified\\\\unified_annotations.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "01_eda_and_split.py\n",
    "Exploratory Data Analysis + Train/Val/Test Split\n",
    "for the unified corrosion dataset\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =============================================================\n",
    "# PATHS\n",
    "# =============================================================\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback for notebooks\n",
    "    ROOT = Path(os.getcwd())\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "UNIFIED_JSON = DATA_DIR / \"processed\" / \"unified\" / \"unified_annotations.json\"\n",
    "RESULTS_DIR = ROOT / \"results\" / \"figures\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# LOAD COCO JSON\n",
    "# =============================================================\n",
    "print(f\"üìÇ Loading unified dataset: {UNIFIED_JSON}\")\n",
    "with open(UNIFIED_JSON, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "images = coco_data[\"images\"]\n",
    "annotations = coco_data[\"annotations\"]\n",
    "categories = coco_data[\"categories\"]\n",
    "print(f\"‚úÖ Loaded {len(images)} images, {len(annotations)} annotations, {len(categories)} categories.\")\n",
    "\n",
    "# =============================================================\n",
    "# EDA: CATEGORY COUNTS\n",
    "# =============================================================\n",
    "cat_counts = defaultdict(int)\n",
    "for ann in annotations:\n",
    "    cat_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "cat_names = {c[\"id\"]: c[\"name\"] for c in categories}\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([cat_names[k] for k in cat_counts.keys()], cat_counts.values(), color=\"steelblue\")\n",
    "plt.title(\"Annotation Counts per Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"data_distributions.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"üìä Saved category distribution plot ‚Üí {RESULTS_DIR / 'data_distributions.png'}\")\n",
    "\n",
    "# =============================================================\n",
    "# VISUALIZE RANDOM SAMPLES\n",
    "# =============================================================\n",
    "def show_random_samples(coco_json_path, num_samples=6):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    images = coco_data['images']\n",
    "    annotations = coco_data['annotations']\n",
    "    categories = {c['id']: c['name'] for c in coco_data['categories']}\n",
    "\n",
    "    img_to_anns = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        img_to_anns[ann['image_id']].append(ann)\n",
    "\n",
    "    samples = random.sample(images, min(num_samples, len(images)))\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for idx, img_info in enumerate(samples):\n",
    "        img_path = Path(img_info.get(\"path\", \"\")) or Path(img_info[\"file_name\"])\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        anns = img_to_anns[img_info[\"id\"]]\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = map(int, ann[\"bbox\"])\n",
    "            color = (255, 0, 0)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cat_name = categories.get(ann[\"category_id\"], \"corrosion\")\n",
    "            cv2.putText(img, cat_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        plt.subplot(2, (num_samples + 1)//2, idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{Path(img_info['file_name']).stem} ({len(anns)} boxes)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"sample_annotations.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "show_random_samples(UNIFIED_JSON, num_samples=6)\n",
    "print(f\"üñºÔ∏è Saved sample visualization ‚Üí {RESULTS_DIR / 'sample_annotations.png'}\")\n",
    "\n",
    "# =============================================================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# =============================================================\n",
    "train_imgs, test_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
    "val_imgs, test_imgs = train_test_split(test_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_imgs,\n",
    "    \"val\": val_imgs,\n",
    "    \"test\": test_imgs\n",
    "}\n",
    "\n",
    "for split_name, split_imgs in splits.items():\n",
    "    img_ids = {img[\"id\"] for img in split_imgs}\n",
    "    split_anns = [a for a in annotations if a[\"image_id\"] in img_ids]\n",
    "    out_data = {\n",
    "        \"images\": split_imgs,\n",
    "        \"annotations\": split_anns,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    out_path = DATA_DIR / \"processed\" / f\"{split_name}_annotations.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(out_data, f, indent=2)\n",
    "    print(f\"üíæ Saved {split_name} set ‚Üí {out_path} ({len(split_imgs)} imgs, {len(split_anns)} anns)\")\n",
    "\n",
    "print(\"‚úÖ EDA and dataset split complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3ba9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Folder structure for: C:\\Users\\Blue\\corrosion_detection\n",
      "============================================================\n",
      "üìÅ config/\n",
      "üìÅ data/\n",
      "‚îÇ   üìÅ organized/\n",
      "‚îÇ   ‚îÇ   üìÅ bmvc_corrosion/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ github_datasets/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ kaggle_pipeline/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ roboflow_corrosao/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ roboflow_inpipe/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ test_samples/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ images/ ... (skipped)\n",
      "‚îÇ   üìÅ processed/\n",
      "‚îÇ   ‚îÇ   üìÅ unified/\n",
      "‚îÇ   üìÅ raw/\n",
      "‚îÇ   ‚îÇ   üìÅ bmvc_corrosion/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_1/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_10/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_2/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_3/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_4/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_5/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_6/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_7/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_8/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cross_val_9/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ DATA_SET_FOR_RELEASE/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ orig_new_name_ALL/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ test/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ github_datasets/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ Phase5_Capstone-Project-main/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ split/\n",
      "‚îÇ   ‚îÇ   üìÅ kaggle_pipeline/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ Pipeline Corrosion Dataset/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ test/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ train/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ roboflow_corrosao/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ train/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ roboflow_inpipe/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ test/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ train/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ valid/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   üìÅ test_samples/\n",
      "üìÅ docs/\n",
      "üìÅ models/\n",
      "‚îÇ   üìÅ checkpoints/\n",
      "‚îÇ   üìÅ configs/\n",
      "üìÅ notebooks/\n",
      "‚îÇ   üìÑ 00_baseline_experiment.ipynb\n",
      "üìÑ requirements.txt\n",
      "üìÅ results/\n",
      "‚îÇ   üìÅ figures/\n",
      "‚îÇ   üìÅ metrics/\n",
      "‚îÇ   üìÅ reports/\n",
      "üìÅ src/\n",
      "‚îÇ   üìÅ data/\n",
      "‚îÇ   ‚îÇ   üìÅ results/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ figures/\n",
      "‚îÇ   üìÅ models/\n",
      "‚îÇ   üìÅ utils/\n",
      "‚îÇ   üìÅ visualization/\n",
      "üìÅ tests/\n",
      "üìÅ venv/\n",
      "‚îÇ   üìÅ etc/\n",
      "‚îÇ   ‚îÇ   üìÅ jupyter/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ nbconfig/\n",
      "‚îÇ   üìÅ Include/\n",
      "‚îÇ   üìÅ Lib/\n",
      "‚îÇ   ‚îÇ   üìÅ site-packages/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ __pycache__/ ... (skipped)\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ _distutils_hack/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ _yaml/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ albucore/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ albucore-0.0.24.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ albumentations/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ albumentations-2.0.8.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ altair/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ altair-5.5.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotated_types/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ annotated_types-0.7.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ attr/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ attrs/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ attrs-25.3.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ blinker/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ blinker-1.9.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cachetools/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cachetools-6.2.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ certifi/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ certifi-2025.8.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ charset_normalizer/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ charset_normalizer-3.4.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cli/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ click/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ click-8.3.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ colorama/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ colorama-0.4.6.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ contourpy/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ contourpy-1.3.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cv2/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cycler/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ cycler-0.12.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ dateutil/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ filelock/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ filelock-3.19.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ fontTools/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ fonttools-4.60.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ fsspec/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ fsspec-2025.9.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ functorch/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ git/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ gitdb/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ gitdb-4.0.12.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ gitpython-3.1.45.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ google/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ huggingface_hub/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ huggingface_hub-0.35.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ idna/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ idna-3.10.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jinja2/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jinja2-3.1.6.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ joblib/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ joblib-1.5.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jsonschema/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jsonschema-4.25.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jsonschema_specifications/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ jsonschema_specifications-2025.9.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ kiwisolver/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ kiwisolver-1.4.9.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ markupsafe/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ markupsafe-3.0.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ matplotlib/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ matplotlib-3.10.6.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ mpl_toolkits/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ mpmath/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ mpmath-1.3.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ narwhals/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ narwhals-2.6.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ networkx/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ networkx-3.5.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ numpy/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ numpy-2.2.6.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ numpy.libs/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ opencv_python-4.12.0.88.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ opencv_python_headless-4.12.0.88.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ packaging/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ packaging-25.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pandas/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pandas-2.3.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pandas.libs/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ PIL/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pillow-11.3.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pip/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pip-24.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pkg_resources/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ protobuf-6.32.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyarrow/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyarrow-21.0.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyarrow.libs/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydantic/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydantic-2.11.9.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydantic_core/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydantic_core-2.33.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydeck/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pydeck-0.9.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyparsing/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyparsing-3.2.5.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ python_dateutil-2.9.0.post0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pytz/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pytz-2025.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ pyyaml-6.0.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ referencing/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ referencing-0.36.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ requests/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ requests-2.32.5.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ rpds/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ rpds_py-0.27.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ safetensors/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ safetensors-0.6.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ scikit_learn-1.7.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ scipy/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ scipy-1.16.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ scipy.libs/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ seaborn/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ seaborn-0.13.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ segmentation_models_pytorch/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ segmentation_models_pytorch-0.5.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ setuptools/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ setuptools-80.9.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ simsimd/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ simsimd-6.5.3.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ six-1.17.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ sklearn/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ smmap/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ smmap-5.0.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ streamlit/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ streamlit-1.50.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ stringzilla-4.1.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ sympy/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ sympy-1.14.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tenacity/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tenacity-9.1.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ threadpoolctl-3.6.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ timm/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ timm-1.0.20.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ toml/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ toml-0.10.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ torch/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ torch-2.8.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ torchgen/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ torchvision/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ torchvision-0.23.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tornado/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tornado-6.5.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tqdm/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tqdm-4.67.1.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ typing_extensions-4.15.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ typing_inspection/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ typing_inspection-0.4.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tzdata/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ tzdata-2025.2.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ urllib3/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ urllib3-2.5.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ watchdog/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ watchdog-6.0.0.dist-info/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ yaml/\n",
      "‚îÇ   üìÅ Scripts/\n",
      "‚îÇ   üìÅ share/\n",
      "‚îÇ   ‚îÇ   üìÅ jupyter/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ nbextensions/\n",
      "‚îÇ   ‚îÇ   üìÅ man/\n",
      "‚îÇ   ‚îÇ   ‚îÇ   üìÅ man1/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CLEAN PROJECT STRUCTURE VIEWER\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "def print_folder_structure(base_dir=None, max_depth=3, skip_heavy_dirs=True):\n",
    "    \"\"\"\n",
    "    Prints the folder structure of the corrosion_detection project.\n",
    "    Automatically detects project root.\n",
    "    Skips deep image folders (train/, test/, valid/, etc.) for clarity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        current = Path.cwd().resolve()\n",
    "\n",
    "    # Detect top-level project folder\n",
    "    while current.name.lower() != \"corrosion_detection\" and current.parent != current:\n",
    "        current = current.parent\n",
    "\n",
    "    base_path = current if base_dir is None else Path(base_dir).resolve()\n",
    "    print(f\"\\nüìÇ Folder structure for: {base_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def recurse(path, depth=0):\n",
    "        if max_depth is not None and depth > max_depth:\n",
    "            return\n",
    "        indent = \"‚îÇ   \" * depth\n",
    "        for item in sorted(path.iterdir()):\n",
    "            if item.name.startswith(\".\"):\n",
    "                continue\n",
    "            if item.is_dir():\n",
    "                # Skip heavy folders to avoid visual clutter\n",
    "                if skip_heavy_dirs and item.name.lower() in {\n",
    "                    \"train\", \"test\", \"valid\", \"images\", \"__pycache__\"\n",
    "                }:\n",
    "                    print(f\"{indent}üìÅ {item.name}/ ... (skipped)\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"{indent}üìÅ {item.name}/\")\n",
    "                recurse(item, depth + 1)\n",
    "            else:\n",
    "                # Only show small number of representative files\n",
    "                if depth <= 1 and item.suffix in {\".py\", \".ipynb\", \".json\", \".txt\"}:\n",
    "                    print(f\"{indent}üìÑ {item.name}\")\n",
    "\n",
    "    recurse(base_path)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print_folder_structure(max_depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a79f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
